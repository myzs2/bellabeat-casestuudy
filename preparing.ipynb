{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab41aadf-4ea9-4e2b-b787-89710dea4b2e",
   "metadata": {},
   "source": [
    "# Raw Data Consolidation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a9c15-32c1-4f3c-a9dd-9eff3b9c0841",
   "metadata": {},
   "source": [
    "The raw data used in this project comes from two separate folders, each covering a specific date range of Fitbit usage:\n",
    "\n",
    "mturkfitbit_export_3.12.16-4.11.16 → Data from Mar 11, 2016 to Apr 11, 2016\n",
    "\n",
    "mturkfitbit_export_4.12.16-5.12.16 → Data from Apr 12, 2016 to May 12, 2016\n",
    "\n",
    "To ensure a continuous and consistent dataset, the consolidation process includes #two main steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0d8aa-0563-4d45-a862-516c16d1c221",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d74c44b-cb90-4b7a-a3a6-fec59e320600",
   "metadata": {},
   "source": [
    "Pandas: for data manipulation and file handling\n",
    "os: for working with file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60bae3f8-8164-409d-ab5d-a65d27f669c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16435647-626d-487e-88fb-8c3850f64f69",
   "metadata": {},
   "source": [
    "# First step : Append Similar Files (Same Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a312a8a-1305-4b5c-97f5-d54036066b66",
   "metadata": {},
   "source": [
    "\n",
    "Files with the same name and identical structure across the two folders were identified and appended together.\n",
    "This step was done because each pair of files represented the same data type but covered different time periods — with one file continuing from where the other left off.\n",
    "Combining them ensures complete time-series without any overlap or loss of continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef131da-bda2-441a-bc71-cd501958212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_daily_full_appended.csv saved.\n",
      "calories_hourly_appended.csv saved.\n",
      "calories_per_minute_narrow_appended.csv saved.\n",
      "heartrate_per_second_appended.csv saved.\n",
      "intensity_hourly_appended.csv saved.\n",
      "intensity_per_minute_narrow_appended.csv saved.\n",
      "mets_per_minute_narrow_appended.csv saved.\n",
      "sleep_per_minute_appended.csv saved.\n",
      "steps_hourly_appended.csv saved.\n",
      "steps_per_minute_narrow_appended.csv saved.\n",
      "weight_log_full_appended.csv saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder1 = 'mturkfitbit_export_3.12.16-4.11.16'\n",
    "folder2 = 'mturkfitbit_export_4.12.16-5.12.16'\n",
    "output_folder = 'appended_data'\n",
    "\n",
    "# These files have matching structures and represent consecutive time periods,\n",
    "# so they are appended to create a full continuous dataset.\n",
    "file_pairs = [\n",
    "    ('activity_daily_full_2016_v01.csv', 'activity_daily_full_2016_v01.csv'),\n",
    "    ('calories_hourly_2016_v01.csv', 'calories_per_hour_2016_v01.csv'),\n",
    "    ('calories_per_minute_narrow_2016_v01.csv', 'calories_per_minute_narrow_2016_v01.csv'),\n",
    "    ('heartrate_per_second_2016_v01.csv', 'heartrate_per_second_2016_v01.csv'),\n",
    "    ('intensity_hourly_2016_v01.csv', 'intensity_per_hour_2016_v01.csv'),\n",
    "    ('intensity_per_minute_narrow_2016_v01.csv', 'intensity_per_minute_narrow_2016_v01.csv'),\n",
    "    ('mets_per_minute_narrow_2016_v01.csv', 'mets_per_minute_narrow_2016_v01.csv'),\n",
    "    ('sleep_per_minute_2016_v01.csv', 'sleep_per_minute_log_2016_v01.csv'),\n",
    "    ('steps_hourly_2016_v01.csv', 'steps_per_hour_2016_v01.csv'),\n",
    "    ('steps_per_minute_narrow_2016_v01.csv', 'steps_per_minute_narrow_2016_v01.csv'),\n",
    "    ('weight_log_full_2016_v01.csv', 'weight_log_summary_2016_v01.csv'),\n",
    "]\n",
    "\n",
    "\n",
    "for file1, file2 in file_pairs:\n",
    "    try:\n",
    "        df1 = pd.read_csv(os.path.join(folder1, file1))\n",
    "        df2 = pd.read_csv(os.path.join(folder2, file2))\n",
    "        combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "        # Drop duplicate rows if any\n",
    "        combined.drop_duplicates(inplace=True)\n",
    "\n",
    "        output_name = file1.replace('_2016_v01.csv', '_appended.csv')\n",
    "        combined.to_csv(os.path.join(output_folder, output_name), index=False)\n",
    "        print(f\"{output_name} saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file1} & {file2}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b60b8-a3ca-4dda-964b-7e19ad5b68bd",
   "metadata": {},
   "source": [
    "#  second step : Standardize Format (Wide to Narrow)  and then append"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17244542-4fc2-4acc-9146-efd57656c57a",
   "metadata": {},
   "source": [
    "This step was necessary because the wide-format files often contained extended time coverage beyond what was available in their narrow-format counterparts. Converting them to the narrow format allowed for consistent structure, making it possible to append the additional records and ensure complete temporal continuity across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2d368-2666-43c4-8012-0f6f058c1a4c",
   "metadata": {},
   "source": [
    "# first step : standardize Format (Wide to Narrow) \n",
    "there is three :\n",
    "(calories_per_minute_wide_2016_v01.csv , intensity_per_minute_wide_2016_v01.csv ,steps_per_minute_wide_2016_v01.csv ) \n",
    "files with wide format we will convert it to narrow format  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b548c3be-0ac9-4af0-9ef4-adaa79da9e34",
   "metadata": {},
   "source": [
    " converting calories_per_minute_wide_2016_v01 file to narrow format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "389a6789-cd34-4281-87fc-60caf8499bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"after_convert\"\n",
    "# Load the wide-format calories data\n",
    "\n",
    "df_cal = pd.read_csv(os.path.join(folder_path, \"calories_per_minute_wide_2016_v01.csv\"))\n",
    "\n",
    "# Unpivot the minute columns into a single 'Minute' and 'Calories' column\n",
    "df_cal_narrow = pd.melt(df_cal,\n",
    "                        id_vars=[\"Id\", \"ActivityHour\"],\n",
    "                        var_name=\"Minute\",\n",
    "                        value_name=\"Calories\")\n",
    "\n",
    "# Extract minute number from column names like 'Minute_0' → 0\n",
    "df_cal_narrow[\"Minute\"] = df_cal_narrow[\"Minute\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Convert the 'ActivityHour' column from string to datetime format\n",
    "df_cal_narrow[\"ActivityHour\"] = pd.to_datetime(df_cal_narrow[\"ActivityHour\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "\n",
    "# Add the 'Minute' offset to the base hour to get the exact timestamp for each record\n",
    "df_cal_narrow[\"ActivityMinute\"] = df_cal_narrow[\"ActivityHour\"] + pd.to_timedelta(df_cal_narrow[\"Minute\"], unit=\"minute\")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "df_cal_narrow = df_cal_narrow[[\"Id\", \"ActivityMinute\", \"Calories\"]]\n",
    "\n",
    "# Save the cleaned and converted dataset to CSV\n",
    "df_cal_narrow.to_csv(os.path.join(output_folder, \"after_convert_calories_per_minute.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f33739c2-172a-4bed-b89c-876875e75c37",
   "metadata": {},
   "source": [
    " converting intensity_per_minute_wide_2016_v01 file to narrow format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aed71744-924d-44da-acc2-97fe3ed6a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"after_convert\"\n",
    "\n",
    "# Load the wide-format intensity data\n",
    "df_int = pd.read_csv(os.path.join(folder_path, \"intensity_per_minute_wide_2016_v01.csv\"))\n",
    "\n",
    "# Unpivot the minute columns into a single 'Minute' and 'Intensity' column\n",
    "df_int_narrow = pd.melt(df_int,\n",
    "                        id_vars=[\"Id\", \"ActivityHour\"],\n",
    "                        var_name=\"Minute\",\n",
    "                        value_name=\"Intensity\")\n",
    "\n",
    "# Extract minute number\n",
    "df_int_narrow[\"Minute\"] = df_int_narrow[\"Minute\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Convert base hour to datetime\n",
    "df_int_narrow[\"ActivityHour\"] = pd.to_datetime(df_int_narrow[\"ActivityHour\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "\n",
    "# Compute the full timestamp per minute\n",
    "df_int_narrow[\"ActivityMinute\"] = df_int_narrow[\"ActivityHour\"] + pd.to_timedelta(df_int_narrow[\"Minute\"], unit=\"minute\")\n",
    "\n",
    "# Keep relevant columns\n",
    "df_int_narrow = df_int_narrow[[\"Id\", \"ActivityMinute\", \"Intensity\"]]\n",
    "\n",
    "# Save the converted dataset\n",
    "df_int_narrow.to_csv(os.path.join(output_folder, \"after_convert_intensity_per_minute.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "304b8fc9-0659-4eee-9c64-30ee3232aa89",
   "metadata": {},
   "source": [
    " converting steps_per_minute_wide_2016_v01 file to narrow format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38bff61d-7cfd-4eee-9a7c-ad36a9ef33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"after_convert\"\n",
    "\n",
    "# Load the wide-format steps data\n",
    "df_steps = pd.read_csv(os.path.join(folder_path, \"steps_per_minute_wide_2016_v01.csv\"))\n",
    "\n",
    "# Unpivot the minute columns into a single 'Minute' and 'Steps' column\n",
    "df_steps_narrow = pd.melt(df_steps,\n",
    "                          id_vars=[\"Id\", \"ActivityHour\"],\n",
    "                          var_name=\"Minute\",\n",
    "                          value_name=\"Steps\")\n",
    "\n",
    "# Extract minute number\n",
    "df_steps_narrow[\"Minute\"] = df_steps_narrow[\"Minute\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Convert base hour to datetime\n",
    "df_steps_narrow[\"ActivityHour\"] = pd.to_datetime(df_steps_narrow[\"ActivityHour\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "\n",
    "# Compute the full timestamp per minute\n",
    "df_steps_narrow[\"ActivityMinute\"] = df_steps_narrow[\"ActivityHour\"] + pd.to_timedelta(df_steps_narrow[\"Minute\"], unit=\"minute\")\n",
    "\n",
    "# Keep relevant columns\n",
    "df_steps_narrow = df_steps_narrow[[\"Id\", \"ActivityMinute\", \"Steps\"]]\n",
    "\n",
    "# Save the converted dataset\n",
    "df_steps_narrow.to_csv(os.path.join(output_folder, \"after_convert_steps_per_minute.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27023670-295a-4ce6-9886-ec720bc9fa86",
   "metadata": {},
   "source": [
    "# second step  : append files   \n",
    "Merge Wide-Converted and Old Narrow Files  \n",
    "This step combines the new wide-format data (after conversion) with existing narrow-format files to ensure complete time coverage for each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03499fd-bf91-4315-9972-070e31eed4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "appended_folder = \"appended_data\"           # Folder containing previously appended narrow-format data\n",
    "converted_folder = \"after_convert\"          # Folder containing newly converted narrow-format data\n",
    "final_folder = \"final_preparing_data\"       # Folder to save the final combined datasets\n",
    "\n",
    "# Create the final output folder if it doesn't exist\n",
    "os.makedirs(final_folder, exist_ok=True)\n",
    "\n",
    "# ======================== Merge Final Calories Data ========================\n",
    "# Load previously appended calories data\n",
    "calories_old = pd.read_csv(os.path.join(appended_folder, \"calories_per_minute_narrow_appended.csv\"))\n",
    "\n",
    "# Load newly converted calories data\n",
    "calories_new = pd.read_csv(os.path.join(converted_folder, \"after_convert_calories_per_minute.csv\"))\n",
    "\n",
    "# Combine old and new calories data\n",
    "calories_final = pd.concat([calories_old, calories_new], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "calories_final = calories_final.drop_duplicates()\n",
    "\n",
    "# Sort by Id and ActivityMinute\n",
    "calories_final = calories_final.sort_values(by=[\"Id\", \"ActivityMinute\"])\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "calories_final.to_csv(os.path.join(final_folder, \"final_calories_per_minute.csv\"), index=False)\n",
    "\n",
    "# ======================== Merge Final Intensity Data ========================\n",
    "# Load previously appended intensity data\n",
    "intensity_old = pd.read_csv(os.path.join(appended_folder, \"intensity_per_minute_narrow_appended.csv\"))\n",
    "\n",
    "# Load newly converted intensity data\n",
    "intensity_new = pd.read_csv(os.path.join(converted_folder, \"after_convert_intensity_per_minute.csv\"))\n",
    "\n",
    "# Combine old and new intensity data\n",
    "intensity_final = pd.concat([intensity_old, intensity_new], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "intensity_final = intensity_final.drop_duplicates()\n",
    "\n",
    "# Sort by Id and ActivityMinute\n",
    "intensity_final = intensity_final.sort_values(by=[\"Id\", \"ActivityMinute\"])\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "intensity_final.to_csv(os.path.join(final_folder, \"final_intensity_per_minute.csv\"), index=False)\n",
    "\n",
    "# ======================== Merge Final Steps Data ========================\n",
    "# Load previously appended steps data\n",
    "steps_old = pd.read_csv(os.path.join(appended_folder, \"steps_per_minute_narrow_appended.csv\"))\n",
    "\n",
    "# Load newly converted steps data\n",
    "steps_new = pd.read_csv(os.path.join(converted_folder, \"after_convert_steps_per_minute.csv\"))\n",
    "\n",
    "# Combine old and new steps data\n",
    "steps_final = pd.concat([steps_old, steps_new], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "steps_final = steps_final.drop_duplicates()\n",
    "\n",
    "# Sort by Id and ActivityMinute\n",
    "steps_final = steps_final.sort_values(by=[\"Id\", \"ActivityMinute\"])\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "steps_final.to_csv(os.path.join(final_folder, \"final_steps_per_minute.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e6bad-b473-484c-826e-c17d5016f355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
