{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c455cb5-587d-43da-91e8-5ce98d1e8d74",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555280f-d98d-4664-96eb-de9a8dff4288",
   "metadata": {},
   "source": [
    "Pandas: for data manipulation and file handling\n",
    "os: for working with file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc95434-98cf-4618-8926-5a46df33c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6190b-5c2c-4287-89d8-ffdb8ea30595",
   "metadata": {},
   "source": [
    "# Processing Phase Overview\n",
    "\n",
    "This notebook documents the complete data processing workflow applied to the prepared datasets.\n",
    "\n",
    "#### Initial Validation and Exploration\n",
    "\n",
    "Before applying any transformations, an initial data exploration was conducted using the custom function `analyze_all_csv()`.  \n",
    "The results of this exploration were saved in dedicated validation notebooks located in the `validation` folder — one for each processing stage.\n",
    "\n",
    "These validation steps helped identify several key issues, including:\n",
    "\n",
    "- Missing values  \n",
    "- Duplicate records  \n",
    "- Outliers in numeric columns  \n",
    "- Inconsistent data types (e.g., date columns stored as strings)  \n",
    "- Columns with irrelevant or unreliable data\n",
    "\n",
    "#### Data Cleaning Actions Based on Validation Insights\n",
    "\n",
    "The insights gathered from the validation reports directly informed the following cleaning steps:\n",
    "\n",
    "- Standardizing date and time formats  \n",
    "- Changing data types for consistency  \n",
    "- Removing invalid or extreme outliers  \n",
    "- Dropping nulls and duplicate rows  \n",
    "- Eliminating columns with unusable values (e.g., the `Fat` column in the weight log)\n",
    "\n",
    "#### Note\n",
    "\n",
    "Some transformation steps — such as wide-to-long restructuring, concatenation, and early duplicate removal — were already applied during the **Preparing Phase**.  \n",
    "These early actions were taken to reduce redundancy, optimize performance, and prepare the data structure for downstream tasks.\n",
    "\n",
    "#### Assurance of Quality and Transparency\n",
    "\n",
    "Each decision in this notebook is supported by evidence from the validation phase.  \n",
    "This structured approach ensures transparency, reproducibility, and integrity throughout the entire data preparation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798627b-ec17-40a8-89a2-6e339e75d207",
   "metadata": {},
   "source": [
    "# Standardizing Datetime Columns Across Datasets\n",
    "\n",
    "To ensure consistency across all datasets, all date-related columns were standardized using a robust multi-format datetime parser.  \n",
    "This parser handled inconsistently formatted strings such as:\n",
    "\n",
    "- `\"4/13/2016 00:00\"`\n",
    "- `\"04/13/2016 12:00 AM\"`\n",
    "- `\"2016-04-13 00:00:00\"`  \n",
    "  etc.\n",
    "\n",
    "Each value was parsed into a proper datetime object and then exported to a unified string format:  \n",
    "**`\"YYYY-MM-DD HH:MM:SS\"`**\n",
    "\n",
    "## Why this step was important:\n",
    "\n",
    "-  **Enabled accurate time-based operations**  \n",
    "  (e.g., sorting, grouping, time-series analysis)\n",
    "\n",
    "-  **Eliminated format inconsistencies**  \n",
    "  across merged datasets\n",
    "\n",
    "-  **Allowed hidden duplicates**  \n",
    "  (same datetime but different formats) to be revealed and addressed\n",
    "\n",
    "-  **Prevented downstream parsing issues**  \n",
    "  by enforcing a reliable and consistent structure\n",
    "\n",
    "##  Technical Note: Datatype vs CSV Limitations\n",
    "\n",
    "Although columns were temporarily converted to `datetime64[ns]` (Pandas' native datetime type), **CSV files do not preserve data types**.  \n",
    "As a result, these columns are saved as plain strings (`object`) in the exported CSVs.\n",
    "\n",
    ">  These datetime columns will be **re-parsed as datetime during the analysis phase** as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e731cb-1b84-4c57-b046-7b7fed95cea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define folder paths\n",
    "input_folder = 'final_prepared_data'\n",
    "output_folder = 'data_after_standardization_and_type_change'\n",
    "\n",
    "# Robust datetime parser with multiple format fallbacks\n",
    "def try_parse_date(date_str):\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\", \"%m/%d/%Y\", \"%m/%d/%Y %I:%M:%S %p\", \"%m/%d/%Y %H:%M:%S\",\n",
    "        \"%m/%d/%Y %I:%M %p\", \"%Y-%m-%d %H:%M:%S\", \"%d/%m/%Y\",\n",
    "        \"%m/%d/%Y %I:%M:%S\", \"%m/%d/%Y %H:%M\"\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.NaT\n",
    "\n",
    "# Files and their datetime columns\n",
    "files = [\n",
    "    ('prepared_activity_daily_full_appended.csv', 'ActivityDate'),\n",
    "    ('prepared_calories_hourly_appended.csv', 'ActivityHour'),\n",
    "    ('prepared_calories_per_minute.csv', 'ActivityMinute'),\n",
    "    ('prepared_heartrate_per_second_appended.csv', 'Time'),\n",
    "    ('prepared_intensity_hourly_appended.csv', 'ActivityHour'),\n",
    "    ('prepared_intensity_per_minute.csv', 'ActivityMinute'),\n",
    "    ('prepared_mets_per_minute_narrow_appended.csv', 'ActivityMinute'),\n",
    "    ('prepared_sleep_per_minute_appended.csv', 'date'),\n",
    "    ('prepared_sleep_summary_daily_2016_v01.csv', 'SleepDay'),\n",
    "    ('prepared_steps_hourly_appended.csv', 'ActivityHour'),\n",
    "    ('prepared_steps_per_minute.csv', 'ActivityMinute'),\n",
    "    ('prepared_weight_log_full_appended.csv', 'Date'),\n",
    "]\n",
    "\n",
    "# Unified string format (date + optional time)\n",
    "def format_date(dt):\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S\") if pd.notna(dt) else \"\"\n",
    "\n",
    "# Process files\n",
    "for filename, column in files:\n",
    "    df = pd.read_csv(os.path.join(input_folder, filename))\n",
    "    df[column] = df[column].apply(try_parse_date).apply(format_date)\n",
    "    df.to_csv(os.path.join(output_folder, filename), index=False)\n",
    "    print(f\"[✓] Saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063e852-ada3-490f-874d-e87744b687a0",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "After inspecting missing values across all datasets using the `isnull().sum()` and percentage calculation functions, I found that the **`Fat` column** in the `prepared_weight_log_full_appended.csv` file contains **94 missing values**, representing **95.92%** of the total entries.\n",
    "\n",
    "Due to the extremely high proportion of missing data in this column (over 95%), and the fact that no other columns across the datasets contain missing values, the `Fat` column was **dropped** from the dataset as it would not contribute meaningfully to the analysis.\n",
    "\n",
    "This decision aligns with standard data cleaning practices, where columns with excessive missing data are considered unreliable for statistical analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9216dfc-1e23-47e7-bab3-a1ba5fd54ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load the file ===\n",
    "df = pd.read_csv('data_after_standardization_and_type_change/prepared_weight_log_full_appended.csv')\n",
    "\n",
    "# === Drop the 'Fat' column ===\n",
    "df.drop(columns=['Fat'], inplace=True)\n",
    "\n",
    "# === Save the cleaned version ===\n",
    "output_folder = 'data_after_removing_nulls_and_duplicates'\n",
    "\n",
    "output_path = os.path.join(output_folder,'prepared_weight_log_full_appendedv2.csv' )\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned file saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abca02-5573-4a94-86c3-88652325bd7e",
   "metadata": {},
   "source": [
    "# Handling Duplicate Records\n",
    "\n",
    "To maintain data integrity and avoid inflated row counts, duplicate records were removed .\n",
    "\n",
    "During the earlier **Prepare** phase, duplicates had already been removed from some concatenated files using `drop_duplicates()`.  \n",
    "This helped reduce file size and improve processing performance.\n",
    "\n",
    "However, after applying consistent datetime formatting in the **Standardization** phase, previously hidden duplicates became detectable.\n",
    "\n",
    "Many datetime columns had been originally stored using inconsistent formats, such as:\n",
    "\n",
    "- `4/13/2016 00:00`  \n",
    "- `04/13/2016 12:00 AM`  \n",
    "- `2016-04-13 00:00:00`\n",
    "\n",
    "While these values represent the same timestamp, their formatting differences caused them to appear unique.  \n",
    "After transforming all datetime values to a unified format (`YYYY-MM-DD HH:MM:SS`), the system was able to correctly identify and remove them as duplicates.\n",
    "\n",
    "This led to an increase in duplicate rows in the following files:\n",
    "\n",
    "- `prepared_calories_per_minute.csv`  \n",
    "- `prepared_intensity_per_minute.csv`  \n",
    "- `prepared_steps_per_minute.csv`\n",
    "\n",
    "Removing these duplicates was essential to maintaining accuracy in time-based metrics and ensuring the reliability of analysis results.\n",
    "\n",
    "The file `prepared_sleep_summary_daily_2016_v01.csv` also contained three residual duplicate rows, which were removed to ensure full consistency.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e5d19-b9b9-4f8d-9ef0-d0d0eea32d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Folder paths ===\n",
    "input_folder = 'data_after_standardization_and_type_change'\n",
    "output_folder = 'data_after_removing_nulls_and_duplicates'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === Files to process ===\n",
    "files_to_deduplicate = [\n",
    "    'prepared_calories_per_minute.csv',\n",
    "    'prepared_intensity_per_minute.csv',\n",
    "    'prepared_steps_per_minute.csv',\n",
    "    'prepared_sleep_summary_daily_2016_v01.csv',\n",
    "]\n",
    "\n",
    "# === Deduplication loop ===\n",
    "for filename in files_to_deduplicate:\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    original_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    removed = original_rows - len(df)\n",
    "\n",
    "    # Save the cleaned file\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"[✓] Processed {filename}\")\n",
    "    print(f\"    → Original rows: {original_rows}\")\n",
    "    print(f\"    → Duplicates removed: {removed}\")\n",
    "    print(f\"    → Final rows: {len(df)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93071880-9b2e-4489-9a83-0cbcee01c21c",
   "metadata": {},
   "source": [
    "# Handling Outliers Using the IQR Method\n",
    "\n",
    "To enhance the quality and reliability of the data, outlier removal was applied to all numeric columns (excluding identifier columns) using the Interquartile Range (IQR) method.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each numeric column in each CSV file:\n",
    "\n",
    "- The interquartile range (IQR) was calculated as:  \n",
    "  `IQR = Q3 - Q1`\n",
    "\n",
    "- Outliers were defined as values outside the following range:\n",
    "  - `Lower bound = Q1 - 1.5 * IQR`\n",
    "  - `Upper bound = Q3 + 1.5 * IQR`\n",
    "\n",
    "- If more than **10%** of the values in a column were identified as outliers, the column was skipped to avoid excessive data loss.\n",
    "\n",
    "## Additional Cleaning Rules\n",
    "\n",
    "- All negative numeric values (i.e., values less than 0) were removed from the dataset, since such values are not expected in this context  \n",
    "  (e.g., negative steps, calories, or active minutes).\n",
    "\n",
    "## Columns Excluded from Outlier Detection\n",
    "\n",
    "- The `Id` column was excluded from all outlier detection processes in order to preserve the integrity of user identifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4143da-c655-4310-92c8-a2082d9f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Folder paths ===\n",
    "input_folder = 'data_after_removing_nulls_and_duplicates'\n",
    "output_folder = 'data_after_removing_outliers'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === Function to remove outliers using IQR ===\n",
    "def remove_outliers_iqr(df, exclude_cols=None, outlier_threshold=10.0):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Count outliers\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_percent = (len(outliers) / len(df)) * 100\n",
    "\n",
    "        # Remove outliers only if % is ≤ threshold\n",
    "        if outlier_percent <= outlier_threshold:\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n",
    "# === Process all CSV files ===\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Drop outliers from numeric columns (except 'Id') and values < 0\n",
    "        df = remove_outliers_iqr(df, exclude_cols=['Id'])\n",
    "\n",
    "        # Remove any numeric values < 0\n",
    "        for col in df.select_dtypes(include=[\"number\"]).columns:\n",
    "            df = df[df[col] >= 0]\n",
    "\n",
    "        # Save cleaned file\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        df.to_csv(output_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
